= Learning transliteration from graphs

Mahdi Mohajeri and Jair Wuilloud
v1.0, 2022-04-0
:doctype: book
:docinfo:

== Introduction

===== General

We have built and demonstrated an innovative strategy allowing for the building
of complex transliteration codes.

We believe that not only our approach makes the work easier, for both developer and
language specialist, but the linguist can easily
(partially or completely) transform the existing solution with little or
 no dependence on the developer.

This was made possible by using data not only to train neural networks but also
treating a linguist's diagrams/graphs as data source for our code.


===== Transliteration for Farsi

Transliteration is the process of transforming a language into another script, transforming the letters such as to maintain the sounds.
For instance: "مزایایی" -> mazAyAyi.

Languages are obviously subject to very complex rules and exceptions, making the above process quite complicated.
It is especially the case if no corpora are available to train neural networks or other techniques on.

In languages where large transliteration corpora are abailable, algorithms
or neural networks can be trained, as in the previous https://github.com/secryst[secryst] work.

However, when no corpus is available as with farsi,
 transliteration data has to be generated. This means in our case
 build a transliteration code from scratch.


===== our Approach & Motivation

Building our initial solution, we faced following problems:

 * Transliteration code based on highly
 in the https://github.com/interscript/transliteration-learner-from-graphs/blob/main/learn-graph/rules/rules.md[complex rules].
 * Language expert and developer's workflow are quite different and
   difficult to integrate.
   For instance, as the developer is asked to encode complex rules he/she is
   likely to have little chances to comprehend.

As a result, not only the code takes a considerable amount of time to be written,
  it has little flexibility for tweaks or redesigns.

For these reasons, we have considered an alternative approach consisting in:

  * Empowering language experts in the process or designing complex rules and logics
  * Encapsulate both language expert and developer workflows
  * Build a flexible solution that can be visualised and tweaked rearranging
   logic blocks


== Strategy

===== Design of diagrams

Experimenting with various approaches, for this project, we decided to work
with https://www.lucidchart.com[lucidchart].
Any technology allowing to create workflows would be an alternative.

===== Building code from graphs

Once exported into a computerisable format, the diagrams can be parsed and
reconstructed as graphs.
We wrote an original code in julia for doing this.


Among the problems that we have to solve:

 0. Modelling of the logics components as Trees
 1. Modelling of data and computation "states"
 2. Jumps from one logic flow to another
 3. Handling of recursions

In a nutshell, our architecture transform data "states" as they "flow"
within the graphical, logical structure.
Details can be found in the https://github.com/interscript/transliteration-learner-from-graphs/tree/main/learn-graph/src[code].

===== Implementation of Nodes/rules

Node elements trigger state data as the latest arrives on one of them.
Code can be found in the link above and more explanation are provided with in
https://github.com/interscript/transliteration-learner-from-graphs/tree/main/learn-graph[learn-graph code].


===== Assets

As mentioned above, mappings are an essential part of the code.
They where transformed into tables in the code.

@Mahdi origin of mapping


===== Data creation and generation

In a first step, we have gathered large amounts of data in farsi:
* datasets

@Mahdi
In a second step, transliteration data was generated applying
our transliteration, diagrams generated code onto the above datasets.


== Learning to transliterate with transformers

===== Transformers

Transformers is a modern neural network architecture
(https://arxiv.org/abs/1706.03762[attention is all your need]) used on transduction problems
such as langugage modelling and translation.
They can be naturally  applied to the problem of learning to transliterate.

Various libraries can be found online. We also experimented with various
 approaches, characters or words based. The currently method is the later.

== Porting python transformers to ruby

===== Training and ONNX  conversion

As for other projects, after training,  ONNX was used to port
 trained neural networks onto a universal format.
This work (training+ONNX export) can be found in
 https://github.com/interscript/transliteration-learner-from-graphs/tree/main/python-nnets-torch[python script]

===== Implementation of greedy decoding

In production, we found that various components (nnets) of the transformers
had to be exported, such as generator, tokenizers, encoder and decoder.

They had then to be combined in a correct fashion in our native ruby code. 



== Discussion

===== Feedback from developers


===== Feedback from Language specialist

@Mahdi


== Summary
