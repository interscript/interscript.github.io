= Learning transliteration from graphs

Mahdi Mohajeri and Jair Wuilloud
v1.0, 2022-04-0
:doctype: book
:docinfo:

== Introduction

==== General

In a context where no transliteration data is available, complex transliteration
codes has to be written from scratch.

We have built an innovative strategy allowing to graphically build
complex transliteration codes.
This approach  encapsulates developer's and linguist's workflows.
It is also allowing the linguist to design and transform complex rules while
allowing for re-usability and visualizations of the whole strategy.

Our strategy is aggressively data based:
data is used not only to train neural networks but also,
our transliteration code is generated with linguist's diagrams/graphs.

The workflow has been successfully applied to Farsi and we think it can-could be ported to and
accelerate any other case where no large transliteration corpus is available.

==== Transliteration for Farsi

Transliteration is the process of transforming a language into another script, transforming the letters to maintain the sounds.
For instance: "مزایایی" -> mazAyAyi.


==== Motivation behind the Approach

Building our initial solution, we faced following problems:

 * Transliteration code had to implement highly
 https://github.com/interscript/transliteration-learner-from-graphs/blob/main/learn-graph/rules/rules.md[complex rules].
 * Language expert and developer's workflows were quite different and
   difficult to integrate.
   For instance, the developer is asked to encode complex rules while he/she is
   likely to have little chance to comprehend,

As a result, not only the code takes a considerable amount of time to be written,
  it has little flexibility for tweaks or redesigns and even debugging.

For that reason, we have considered an alternative approach consisting in:

  * Empowering language experts in the process of designing complex rules and logic
  * Encapsulating both language expert and developer workflows
  * Building a flexible solution that can be visualized and tweaked by rearranging
   logic blocks

Furthermore, as the production is done in javascript from Ruby code,
 key libraries do not exist in Ruby and would need to be retro-engineered.
Fortunately, neural network can be used to learn complex rules if creating
large good quality datasets.
We could in part rely there on some of the team's
 previous work  https://github.com/secryst[secryst].


== Strategy

In a nutshell:

1. *Transliteration design*
  * linguist designs code diagrams
  * nodes implemented by developer
  * => code automatically generated/ "learnt" from above step

2. *Learning of transliteration with NNets*
  * => transliteration dataset creation  with above code
  * training of NNets

3. *Production/Transliteration in Ruby*
  * torch NNets => export in ONNX
  * code ran in ruby


==== Design of diagrams

Link for and example of a
 https://github.com/interscript/transliteration-learner-from-graphs/blob/main/learn-graph/resources/Model1.0.png[diagram]
 representing a strategy designed by Mahdi.

Experimenting with various approaches for this project, we decided to work
with https://www.lucidchart.com[lucidchart].
However, any technology allowing to create workflows could be an
 alternative and massaged.

==== Building code from graphs

Once exported into a csv format, the diagrams can be computerized and
 modelled as trees, associating  rules to each node.


Among the problems that had to be solved:

 1. Modelling of logic components as Trees
 2. Modelling of computation "states"
 3. Jumps from one logic flow to another
 4. Handling recursions

We are using a functional approach with the
data "states" getting transformed as they "flow"
within the graphical, logical structure.
Details can be found in the original
 https://github.com/interscript/transliteration-learner-from-graphs/tree/main/learn-graph/src[Julia code].

====  Nodes/Rules

Nodes are associated to an action to be performed that is described in plain English.
For instance, in the https://github.com/interscript/transliteration-learner-from-graphs/blob/main/learn-graph/resources/Model1.0.png[diagram] example:

* node|> "output its transliteration!"
* node|> "is the prefix ب or بی?"
* node|> "is it a verb?"
* node|> "transliterate it using affix-handler"


Our code can be found with more explanations here:
https://github.com/interscript/transliteration-learner-from-graphs/tree/main/learn-graph[learn-graph code].

The current code has over 150 nodes and is a successful implementation of a
complex solution!

==== Assets used & created

===== Mappings

As mentioned above, mappings are an essential part of the code.
They were transformed into tables in the code.
Our rules combine NLP tags to break down words into
smaller segments for which a transliteration is found in the database.

The main database we used for this project mappings was a 50k-word
database used previously in a Farsi text to speech project called
https://www.yasdl.com/tag/parskhan[Parskhan].
It includes word roots and their frequency in conversations and affixes that
can be attached to those roots.
That database had to be edited in multiple occasions.



===== Data creation and generation

Most of our datasets to
apply our transliteration method on were publicly available from Farsi NLP
 communities and Github repositories.

As a second step, transliteration data was generated by applying
our  diagrams-generated code onto the above datasets.

We have also produced a small test dataset to benchmark various transliteration
algorithms. With this data, we have tried to cover many  cases our
rules were designed to solve.


==== NLP in Farsi

After some research, we decided to use https://github.com/sobhe/hazm[hazm library].
It is available only in python but we could use neural networks to bypass this issue
 for production, as explained below.


==== Workflow Details for Linguist & Developer


While the developer's job consists in the implementation of the above commands
(searches in tables, comparing and concatenating strings, ...),
the linguist can produce various nodes with commands
and organize/re-organize them on the graphical editor.

In more details:

1. starting from a diagram the linguist can use a graphical editor to
design various rules

2. If a new node has to be created interact with a developer to implement it.

3. Learn build code from graphs

4. run test and benchmarks and review results and bugs

5.  run single examples with an extensive debugging mode

6. back to 1.


Below, we show the code output in full verbose, debug mode.
The linguist can track the computation steps and help to identify bugs and inaccuracies.
[source,sh]
----
> julia transliterateSingleString.jl --path-model resources/Model0.9.dat --farsi-text یویو --pos-tagging noun
[ Info: ("brain name ::> ", "transliterator")
[ Info: ("data::> ", Dict{String, Any}("brain" => "transliterator", "pos" => "Noun", "word" => "یویو", "pre_pos" => nothing, "state" => nothing))
[ Info: ("node::> ", "change all instances of ي and ك in the text to ی and ک")
[ Info: ("data::> ", Dict{String, Any}("brain" => "transliterator", "pos" => "Noun", "word" => "یویو", "pre_pos" => nothing, "state" => nothing))
[ Info: ("node::> ", "is the word found in the db?")
[ Info: ("response::> ", "yes")
[ Info: ("data::> ", Dict{String, Any}("brain" => "transliterator", "data" => Dict{Any, Any}[Dict("الگوی تکیه" => "WS", "WrittenForm" => "یویو", "PhonologicalForm" => "yoyo", "Freq" => 1, "SynCatCode" => "N1")], "pos" => "Noun", "word" => "یویو", "pre_pos" => nothing, "state" => "yes"))
[ Info: ("node::> ", "collision?")
[ Info: ("response::> ", "no")
[ Info: ("data::> ", Dict{String, Any}("brain" => "transliterator", "data" => Dict{Any, Any}[Dict("الگوی تکیه" => "WS", "WrittenForm" => "یویو", "PhonologicalForm" => "yoyo", "Freq" => 1, "SynCatCode" => "N1")], "pos" => "Noun", "word" => "یویو", "pre_pos" => nothing, "state" => "no"))
[ Info: ("node::> ", "output its transliteration!")
yoyo
----


== Learning to transliterate with transformers

==== Transformers

Transformers are a modern neural network architecture
(https://arxiv.org/abs/1706.03762[attention is all you need]) used on transduction problems
such as language modeling and translation.
They can be naturally applied to the problem of learning to transliterate.

Various libraries can be found online. We also experimented with multiple
 approaches, characters or words-based. The current method implemented in
 production is the latter.

Several resources are available online to
 https://jalammar.github.io/illustrated-transformer/[explain transformers].

// ==== Porting python transformers to ruby

==== Training and ONNX conversion

As for other projects, after training, ONNX was used to port
 trained neural networks onto a universal format.
This work (training+ ONNX export) can be found in
 https://github.com/interscript/transliteration-learner-from-graphs/tree/main/python-nnets-torch[python script].

==== Implementation of greedy decoding

In production, we found that various components (neural networks) of the transformers
had to be exported, such as generator, tokenizers, encoder, and decoder.

They had then to be combined correctly in our native
https://github.com/interscript/transliteration-learner-from-graphs/tree/main/lib[ruby code].

== Benchmarking


==== Scores

Our codes can be tested/benchmarked with a test data set that we have designed.
We are reporting ACCU as (word accuracy %):

[cols="a,a",options="header"]
|===
| |ACCU

|*CODE 0.9* |96%
|*CODE D* |in Progress%

|*CODE Transfo* |65%
|*CODE Ruby* |60%

|===

* *CODE 0.9* is our first transliteration code.
It has been optimized on our test set and after quite some work,
could reach a very decent score.
However, the code does not not cover/fails with many sentence (50%).

* *CODE D* is the code based on diagrams

* *CODE Transfo* is the code trained with transformer

* *CODE Ruby* is the final, production code

The discrepancy between the scores of *CODE 0.9* and *CODE Transfo*
is caused, we think, by cases that *CODE 0.9* can not encode properly.
Because the code outputs farsi characters when not knowing how to handle a
characters sequence, 1/10 words are transliterated with some farsi in it.
These had to be filtered out and therefore,
 ~1/2 of the sentences could not be transliterated. We think that this impacted
 the transliteration dataset quality.

This also motivated the rewriting of a new version of *CODE 0.9*, which in
 turn, because of its difficulty/challenge, led to the alternative graphical
  approach featured in this blog.





== Summary & Discussion

For reasons explained in benchmarks and in the introduction,
we found impractical and sub-efficient to build a transliteration
code from a
set of mappings and written rules.

Thinking that the integration between software developer and
linguist was one of the challenges, with difficulties for the
former to develop some sort of intuition about a foreign language
and the latter to debug or
implement himself tweaks or changes,
we have approached the problem with a graphical
editor allowing a linguist to creates his own logic designs.

Transliteration is put into production after training of neural networks,
allowing to bypass the usage of NLP libraries not available in ruby
but also for a compact solution.

In the final step, we found a lightweight way to export torch transformers
into native ruby, without using more than very standard libraries (no torch-rb).

We think that the approach or part of it can be ported to the transliteration of
any other languages, also the ones where no transliteration data is available.

After having demonstrated its application to a complex software implementation,
we also think that the graphical approach and allowing for a good encapsulation
of technical and specialist workflow can be very useful in many situations.

Several new technologies suggest many ideas to scale up the approach, for instance
https://copilot.github.com/[AI pair programmer].



// == Next Steps



// ==== Feedback from developer

//@Jair

//==== Feedback from Language specialist

//@Mahdi
