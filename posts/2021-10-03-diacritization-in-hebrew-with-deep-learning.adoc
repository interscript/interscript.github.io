= Diacritization in Hebrew with Deep Learning
Jair Wuilloud
v1.0, 2021-08-03
:doctype: book
:docinfo:

== Introduction

=== General



After having written a solution able to point arabic textes
and ported it to production with our library
https://github.com/interscript/rababa[Rababa].

This is a technical blog about the excellent results we obtained
when applying the "rababa strategy and architecture" to hebrew.

We explain how we have managed *to half
the decision errors recorded on the automatised diacritization*
of Abjad languages.


=== Role of diacritization

Quite often and in modern texts,
hebrew is written using the hebrew alphabet.
This script just specifies the consonants, leaving the
wovels to be left to the reader to infer.


*Diacritization*, Pointing or Niqqud is the hebraic system used to
specify these wovels and is found in liturgic hebrew and
in schools.

Cantillation is another set of symbols added to Hebrew texts within the
religious context.  It indicates how hebrew should be chanted.
Cantillation is not covered here.

image::/figs/diacritization+cantillation.png[Example of Hebraic diacritization(blue)
with cantillation(red)]


=== Applications in informatics

Pointed Hebrew is obviously useful and essential for applications like
 *text-to-speech* and *transliteration*.

=== Niqqud

The Niqqud is the set of symbols used to enrich Hebrew
and specify the vocalization. We present here the classification used we have overtaken
from  https://github.com/elazarg/nakdimon/blob/master/hebrew.py[nakdimon hebrew code]:

* *NIQQUD* : shva,reduced segol,reduced patakh,
             reduced kamatz,hirik,tzeire,segol,
             patakh,kamatz,holam,kubutz,shuruk,meteg
* *DAGESH* :  rafe,dagesh letter,mappiq
* *SIN* : rafe,shin yemanit,shin smalit

The rationale of the above decomposition is that sin is present only on the sin letter
of the alphabet. Dagesh is placed in the middle of the letters.
Member of the Niqqud presents themselves as "pixels" on the top or bottom of
the alphabets.

As a result, only one member of each of the above group
 can enrich a letter.  But a letter could for instance be
decorated with one member of NIQQUD and DAGESH classes.
Therefore, the above decomposition allows to represent all the symbols
in three independent space.


== Diacritization in Interscript

=== General

Interscript provides with mappings allowing to transliterate many languages into
various writing systems.

In this context, Abjad languages need to be processed via several steps:

* *Hebraic text* => *diacritization* => *transliteration*

=== Diacritization with Deep Learning

Correct diacritization requires an accurate understanding not only of the
language morphemes and their variants but also the language grammar.

Furthermore, given the possible multiple meanings available to a particular word
in Hebrew (or collision), some understanding of the context is required!

=== Approaches

This hard problem has been approached in various ways with an evolution quite
typical:

. Rule-based approaches
. Machine Learning approaches
. Deep Learning approaches

For more details, we have reviewed the latest publications, tested the latest
code bases and summarised the latest research ideas
https://github.com/interscript/rababa/blob/main/docs/research-arabic-diacritization-06-2021.adoc[here].


=== Literature Review

* *NAKDAN* :
https://arxiv.org/pdf/2005.03312.pdf[Nakdan (2020)]
A https://nakdanpro.dicta.org.il/[live system] based on 3 steps
  combining engineered linguistic informationand a trained neural model:
  1. Part of Speech Tagging
  2. Filtering the possible diacritizations
  3. Ranking the possible diacritizations for eachword, in context

* *NAKDIMON* :
https://arxiv.org/pdf/2105.05209.pdf[Nakdimon (2021)]
Nakdimon is a lighter system attempting to achieve diacritization by
using a more powerful Neural Network model only.

The authors have also published a new data set, codes for NLP and metrics
that we have largely overtaken in our work.

=== Our Architecture

Our architecture is based on the Tacotron and CBHG's, as
explained in our recent work on arabic diacritization
https://www.interscript.org/blog/2021-08-03-diacritization-in-arabic-with-deep-learning[blog].

== Training and results


==== NLP of Hebrew Diacritics
We have provided a heuristic explanation of the
decomposition of  Diacritics as: NIQQUD/SIN/DAGESH.

We also found much better results against a naive attempt where all the diacritics
would cohexist within the same space.

==== Modelling of Niqqud, Sin \& Dagesh

Compared to the architecture described in our previous blog,
the simplest change was to just add 2 additional
CBHG projections to the model.

The model is then trained
backpropagating in a serial fashion from the Niqqud/Dagesh and Sin
 projection losses
(https://github.com/interscript/rababa/tree/hebrew[hebrew code]).




=== Datasets
The original dataset was taken from
https://github.com/elazarg/hebrew_diacritized[Hebrew Diacritized]

The dataset contains a range of diacritized textes of multiple origins:
ancient, religious, modern, poetry, ...

The datasets needed some cleaning and we are going to publish
those cleaned datasets very soon.

=== Training Strategies

==== Code to run Experiments
We have integrated the code with https://wandb.ai/[Wandb]
to make it simpler to run extensive experiments and monitor/show
 results in real time.

==== Experiments with Datasets
The variaty and diacritization quality within the datasets allowed
 to run multiple experiments.

We found that to pre-training Rababa
first with various datasets before using the modern
hebrew corpus as target would slightly increase the results.

This will discussed in more details very soon.

==== Hyperparams Tuning

On the top of the datasets, various parameters can be fine tuned.

We have tried and evaluated various combinations, which will also be discussed in more details very soon.


=== System Evaluation and Performance

* *DEC*: decision accuracy
* *CHA*: character accuracy
* *WOR*: word accuracy
* *VOC*: vocalization accuracy

In order to make sure our metrics are correct,
we have overtaken nakdimon code and checked that we could
reproduce exactly their results on their test dataset.

==== Scores after Training

We refer to https://arxiv.org/pdf/2105.05209.pdf[paper], table 3 for
our performance comparison below.

[cols="a,a,a,a,a",options="header"]
|===
| |DEC |CHA |WOR |VOC
|*Nakdan* |98.94|98.23|95.83 |  95.93
|*Nakdimon* |97.37 |95.41 |87.21 |89.32
|*Rababa* |*99.63* |*99.33* |*97.58* | *98.18*
|===


Rababa is our best run. We remind here that we are comparing against Nakdan,
which is a hybrid (nnets + rules + search),
Nakdimon is nnets only.

Evidences can be found in exploring the following
https://wandb.ai/jair/hebrew-diacritization-play/reports/Runs-result--VmlldzoxMDUyMzE0?accessToken=34s286jvk0j1ozwzn02mlrmfkvmlyc07en34dkeh7u7fx9cphuo2j8u8v4jn0fze[link].


== Discussion & Summary

=== Discussion
Not only we could adapt rababa to the problem of hebrew diacritization,
but using the good work made on NLP, modelling and the datasets
published by other team, we could beat their benchmarks
 by as substantial margin.

We confirmed that as mentionned in the 2021, nakdimon paper,
we could improve the results by pre-training on ancient or religious Datasets
previous to targetting a smaller, modern one.



While our reached decision accuracy DEC<0.4 is to be compared with
previous scores of DEC>1.0% for hybrid systems in Hebrew,
Arabic diacritization is currently around 0.85% when comparing a similar
metric (DER*).

=== Summary


As a consequence,
we have halfed the decision error recorded on the automatised
diacritization of Abjad languages, thus *entering into a new domain for what
deep learning can achieve within that space.*
